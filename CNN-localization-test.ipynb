{
 "cells": [
  {
   "cell_type": "raw",
   "id": "51765d86",
   "metadata": {},
   "source": [
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9bffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from src.utils.image_stuff import image_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516da4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(data)  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d38ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966d2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model34_2.pth')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f3ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 500\n"
     ]
    }
   ],
   "source": [
    "dataset_dir_path = \"/home/hue/Codes/AIROGS/datasets/5_normalized_224x224\"\n",
    "images_path = [os.path.join(dataset_dir_path,f) for f in sorted(os.listdir(f'{dataset_dir_path}'))][:500]\n",
    "print(f\"Total number of images: {len(images_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f617cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "outout_dir_path = \"/home/hue/Codes/AIROGS/datasets/CNN_v2_output\"\n",
    "if not os.path.exists(outout_dir_path):\n",
    "    os.mkdir(outout_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708e2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef43aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(images_path):\n",
    "    for path in images_path:\n",
    "        yield [Image.open(path), os.path.basename(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bccb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = read_images(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a542ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e538e7cd5046e4b0cc094aa0de4a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hue/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/434287/what-is-the-most-pythonic-way-to-iterate-over-a-list-in-chunks/434328\n",
    "from iteration_utilities import grouper\n",
    "\n",
    "\n",
    "for group in tqdm(grouper(images, 100), total=len(images_path)//100):\n",
    "# https://stackoverflow.com/questions/67814465/convert-list-of-tensors-into-tensor-pytorch\n",
    "    data = torch.stack([transform(image[0]) for image in group]).to(device)\n",
    "    output = predict(model, data)\n",
    "    \n",
    "    for idx in range(data.shape[0]):\n",
    "        bbox_image = image_stuff.draw_bbox(np.array(group[idx][0]),\n",
    "                                           output.data.int()[idx],\n",
    "                                           \"deep_learning\",\n",
    "                                           box_color=(0, 225, 0))\n",
    "        \n",
    "        cv2.imwrite(os.path.join(outout_dir_path,f\"{group[idx][1][:-4]}_bbox.jpg\"),\n",
    "                    cv2.cvtColor(bbox_image, cv2.COLOR_RGB2BGR))\n",
    "        with open(os.path.join(outout_dir_path, f\"{group[idx][1][:-4]}.txt\"), \"w\") as f:\n",
    "            for coordinate in output.data.int()[idx]:\n",
    "                f.write(f\"{int(coordinate)}\\n\") \n",
    "\n",
    "#         image_stuff.plt_display_image(bbox_image, figsize=(10,10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83ec52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
